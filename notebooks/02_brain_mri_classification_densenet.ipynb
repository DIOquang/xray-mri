{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204d4785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, sys, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('TensorFlow:', tf.__version__)\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c8de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths & Hyperparameters\n",
    "BASE_PATH = '../data/brain_mri/'\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
    "VAL_PATH = os.path.join(BASE_PATH, 'val')\n",
    "TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
    "for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
    "    assert os.path.isdir(p), f'Missing directory: {p}'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE_1 = 15  # keep moderate for demo; adjust as needed\n",
    "EPOCHS_PHASE_2 = 15\n",
    "MODELS_DIR = '../models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c2a5f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m train_datagen = ImageDataGenerator(\n\u001b[32m      3\u001b[39m     rescale=\u001b[32m1.\u001b[39m/\u001b[32m255\u001b[39m,\n\u001b[32m      4\u001b[39m     rotation_range=\u001b[32m20\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     fill_mode=\u001b[33m'\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m val_test_datagen = ImageDataGenerator(rescale=\u001b[32m1.\u001b[39m/\u001b[32m255\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_generator = \u001b[43mtrain_datagen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategorical\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m validation_generator = val_test_datagen.flow_from_directory(\n\u001b[32m     16\u001b[39m     VAL_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode=\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m test_generator = val_test_datagen.flow_from_directory(\n\u001b[32m     19\u001b[39m     TEST_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode=\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/xray-mri/.venv/lib/python3.13/site-packages/keras/src/legacy/preprocessing/image.py:1136\u001b[39m, in \u001b[36mImageDataGenerator.flow_from_directory\u001b[39m\u001b[34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[39m\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflow_from_directory\u001b[39m(\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1120\u001b[39m     directory,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1134\u001b[39m     keep_aspect_ratio=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1135\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/xray-mri/.venv/lib/python3.13/site-packages/keras/src/legacy/preprocessing/image.py:484\u001b[39m, in \u001b[36mDirectoryIterator.__init__\u001b[39m\u001b[34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[39m\n\u001b[32m    482\u001b[39m classes_list = []\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     classes, filenames = \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     classes_list.append(classes)\n\u001b[32m    486\u001b[39m     \u001b[38;5;28mself\u001b[39m.filenames += filenames\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    123\u001b[39m job, i, func, args, kwds = task\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/xray-mri/.venv/lib/python3.13/site-packages/keras/src/legacy/preprocessing/image.py:200\u001b[39m, in \u001b[36m_list_valid_filenames_in_directory\u001b[39m\u001b[34m(directory, white_list_formats, split, class_indices, follow_links)\u001b[39m\n\u001b[32m    197\u001b[39m     classes.append(class_indices[dirname])\n\u001b[32m    198\u001b[39m     absolute_path = os.path.join(root, fname)\n\u001b[32m    199\u001b[39m     relative_path = os.path.join(\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         dirname, \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsolute_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     )\n\u001b[32m    202\u001b[39m     filenames.append(relative_path)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m classes, filenames\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:512\u001b[39m, in \u001b[36mrelpath\u001b[39m\u001b[34m(path, start)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:384\u001b[39m, in \u001b[36mabspath\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True\n",
    ")\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False\n",
    ")\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_PATH, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False\n",
    ")\n",
    "num_classes = train_generator.num_classes\n",
    "class_indices = train_generator.class_indices\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "print('Classes:', class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Weights\n",
    "classes_unique = np.unique(train_generator.classes)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=classes_unique, y=train_generator.classes)\n",
    "class_weights = {int(k): float(v) for k, v in zip(classes_unique, cw)}\n",
    "print('Class Weights:', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model (Phase 1 - frozen backbone)\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13740fe",
   "metadata": {},
   "source": [
    "### Phase 1 Training - Transfer Learning\n",
    "Train only classification head until validation stabilizes or max epochs reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76490a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks Phase 1\n",
    "best_phase1 = os.path.join(MODELS_DIR, 'brain_mri_densenet_phase1_best.h5')\n",
    "ckpt_each_p1 = os.path.join(MODELS_DIR, 'brain_mri_phase1_epoch_{epoch:02d}_valacc_{val_accuracy:.4f}.h5')\n",
    "callbacks_p1 = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(best_phase1, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(ckpt_each_p1, monitor='val_accuracy', save_best_only=False, verbose=0)\n",
    "]\n",
    "history_p1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE_1,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_p1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38a19d",
   "metadata": {},
   "source": [
    "### Phase 2 Training - Fine-tuning\n",
    "Unfreeze upper half of DenseNet-121 and continue training with lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning setup\n",
    "base_model.trainable = True\n",
    "fine_tune_at = len(base_model.layers) // 2\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "best_phase2 = os.path.join(MODELS_DIR, 'brain_mri_densenet_phase2_best.h5')\n",
    "ckpt_each_p2 = os.path.join(MODELS_DIR, 'brain_mri_phase2_epoch_{epoch:02d}_valacc_{val_accuracy:.4f}.h5')\n",
    "callbacks_p2 = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(best_phase2, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    ModelCheckpoint(ckpt_each_p2, monitor='val_accuracy', save_best_only=False, verbose=0)\n",
    "]\n",
    "history_p2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE_1 + EPOCHS_PHASE_2,\n",
    "    initial_epoch=len(history_p1.history['loss']),\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_p2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfe0e9",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}%')\n",
    "probs = model.predict(test_generator, verbose=0)\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=[idx_to_class[i] for i in range(num_classes)])\n",
    "print(report)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[idx_to_class[i] for i in range(num_classes)], yticklabels=[idx_to_class[i] for i in range(num_classes)])\n",
    "plt.title('Confusion Matrix - Brain MRI')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c6ec8",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_p1.history['accuracy'] + history_p2.history['accuracy']\n",
    "val_acc = history_p1.history['val_accuracy'] + history_p2.history['val_accuracy']\n",
    "loss = history_p1.history['loss'] + history_p2.history['loss']\n",
    "val_loss = history_p1.history['val_loss'] + history_p2.history['val_loss']\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(acc, label='Train Acc')\n",
    "plt.plot(val_acc, label='Val Acc')\n",
    "plt.axvline(x=EPOCHS_PHASE_1-1, color='r', linestyle='--', label='Fine-tune start')\n",
    "plt.legend(); plt.title('Accuracy'); plt.xlabel('Epoch')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Val Loss')\n",
    "plt.axvline(x=EPOCHS_PHASE_1-1, color='r', linestyle='--', label='Fine-tune start')\n",
    "plt.legend(); plt.title('Loss'); plt.xlabel('Epoch')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779d47e",
   "metadata": {},
   "source": [
    "### Grad-CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0917a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Grad-CAM for a few test samples\n",
    "def get_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "# find last conv layer\n",
    "last_conv = None\n",
    "for layer in model.layers[::-1]:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        last_conv = layer.name; break\n",
    "if last_conv is None:\n",
    "N = min(3, len(test_generator.filenames))\n",
    "for i in range(N):\n",
    "    batch = test_generator[i]\n",
    "print('Grad-CAM done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242edb03",
   "metadata": {},
   "source": [
    "### Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_h5 = os.path.join(MODELS_DIR, 'brain_mri_densenet_final.h5')\n",
    "model.save(final_model_h5)\n",
    "print('Saved:', final_model_h5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
